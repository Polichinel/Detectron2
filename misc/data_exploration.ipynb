{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if local use new_torch_env\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from xml.etree import ElementTree, ElementInclude\n",
    "\n",
    "import pickle\n",
    "\n",
    "from detectron2.structures import BoxMode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(42) # see if this is the culprit.\n",
    "\n",
    "def get_classes(img_dir):\n",
    "    \"\"\"Creates a list of classes and corrosponding ints. also a dict to translate\"\"\"\n",
    "\n",
    "    obj_name = []\n",
    "\n",
    "    # Get all objects that have been annotated\n",
    "    for filename in os.listdir(img_dir):\n",
    "        if filename.split('.')[1] == 'xml':\n",
    "            box_path = os.path.join(img_dir, filename)\n",
    "\n",
    "            tree = ElementTree.parse(box_path)\n",
    "            lst_obj = tree.findall('object')\n",
    "\n",
    "            for j in lst_obj:\n",
    "                obj_name.append(j.find('name').text)\n",
    "    \n",
    "    classes = list(sorted(set(obj_name))) # all labesl\n",
    "    classes_int = list(np.arange(0,len(classes))) # corrospoding int\n",
    "    class_to_int = dict(zip(classes,classes_int)) # a dict to translate between them\n",
    "\n",
    "    return(classes, classes_int, class_to_int)\n",
    "\n",
    "\n",
    "def get_img_path(img_dir):\n",
    "\n",
    "    \"\"\"Creates a list of all image paths.\"\"\"\n",
    "\n",
    "    # right now this does not take into account whether the image was anotated or not.\n",
    "    # It also does not handle test or train.\n",
    "\n",
    "    img_path_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(img_dir):\n",
    "        for img_name in files:\n",
    "            if img_name.split('.')[1] == 'jpg':\n",
    "                img_path = os.path.join(img_dir, img_name)                \n",
    "                img_path_list.append(img_path)\n",
    "\n",
    "    return(img_path_list)\n",
    "\n",
    "\n",
    "def get_annotation_path(img_dir):\n",
    "\n",
    "    \"\"\"Creates a list of all box paths.\"\"\"\n",
    "\n",
    "    annotation_list = []\n",
    "\n",
    "    for filename in os.listdir(img_dir):\n",
    "        if filename.split('.')[1] == 'xml':\n",
    "            annotation_list.append(filename)\n",
    "\n",
    "    return(annotation_list)\n",
    "\n",
    "def get_train_test(annotation_list, train_ratio = 0.8):\n",
    "\n",
    "    train_n = int(len(annotation_list) * train_ratio)\n",
    "    train_set = np.random.choice(annotation_list, train_n, replace = False)\n",
    "    test_set = [i for i in annotation_list if i not in train_set]\n",
    "\n",
    "    return(train_set, test_set)\n",
    "\n",
    "\n",
    "def get_img_dicts(img_dir, train = True):\n",
    "\n",
    "    _, _, class_to_int = get_classes(img_dir) # only need the dict here.\n",
    "    annotation_list = get_annotation_path(img_dir) # new\n",
    "    train_set, test_set = get_train_test(annotation_list) \n",
    "\n",
    "    dataset_dicts = []\n",
    "    idx = 0\n",
    "\n",
    "    # if you just want a list to go through, you cna generalizr the function below (get_img_path)... \n",
    "    # and if you had that function splitting into train and test would be simple.\n",
    "\n",
    "    if train == True:\n",
    "        subset = train_set\n",
    "    \n",
    "    elif train == False:\n",
    "        subset = test_set\n",
    "\n",
    "    # here you need to think about val (you use test right now) and also the full set...\n",
    "\n",
    "    for filename in subset:\n",
    "\n",
    "    # for filename in os.listdir(img_dir):\n",
    "    #    if filename.split('.')[1] == 'xml': # only for annotated images. filename is now effectively annotationes.\n",
    "\n",
    "        img_name = filename.split('.')[0] + '.jpg' # the image name w/ correct extension.\n",
    "        \n",
    "        record = {}\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "\n",
    "        height, width = cv2.imread(img_path).shape[:2]\n",
    "\n",
    "        record[\"file_name\"] = img_path #  needs to be the full path to the image file acccording to docs.\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "\n",
    "        objs = []\n",
    "        obj_path = os.path.join(img_dir, filename)\n",
    "        tree = ElementTree.parse(obj_path)\n",
    "\n",
    "        annotations = tree.findall('object')\n",
    "\n",
    "        for i in annotations: # go through all annotated objs in a given image\n",
    "\n",
    "            label = i.find('name').text # get the label\n",
    "            box = i.findall('bndbox') # find the box\n",
    "\n",
    "            for j in box: # get the 4 measures from the box\n",
    "\n",
    "                xmin = float(j.find('xmin').text) \n",
    "                xmax = float(j.find('xmax').text) \n",
    "                ymin = float(j.find('ymin').text)\n",
    "                ymax = float(j.find('ymax').text) \n",
    "\n",
    "            obj = { 'bbox': [xmin, ymin, xmax, ymax],\n",
    "                    'bbox_mode': BoxMode.XYXY_ABS, # remember to change!\n",
    "                    'category_id': class_to_int[label],\n",
    "                    'catagory_label': label,\n",
    "                    'iscrowd' : 0}\n",
    "\n",
    "            objs.append(obj)\n",
    "\n",
    "        record[\"annotations\"] = objs\n",
    "\n",
    "        dataset_dicts.append(record)\n",
    "        idx += 1\n",
    "        print(idx, end=\"\\r\")\n",
    "  \n",
    "    return(dataset_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\r"
     ]
    }
   ],
   "source": [
    "img_dir = '/home/simon/Documents/Bodies/data/jeppe/images' #'/home/projects/ku_00017/data/raw/bodies/OD_images_annotated' \n",
    "\n",
    "train_img_dicts = get_img_dicts(img_dir, train = True)\n",
    "val_img_dicts = get_img_dicts(img_dir, train = False)\n",
    "\n",
    "# classes = get_classes(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_df(img_dicts):\n",
    "\n",
    "    list_of_counts = []\n",
    "\n",
    "    for i in img_dicts:\n",
    "\n",
    "        list_of_annotations = []\n",
    "        for j in i['annotations']:\n",
    "\n",
    "            list_of_annotations.append(j['catagory_label'])\n",
    "\n",
    "        dict_of_counts_a = {'image_id' : i['image_id'], 'file_name': i['file_name']}\n",
    "\n",
    "        dict_of_counts_b = dict(Counter(list_of_annotations))\n",
    "\n",
    "        dict_of_counts = {**dict_of_counts_a, **dict_of_counts_b} # merge dicts\n",
    "\n",
    "        list_of_counts.append(dict_of_counts)\n",
    "\n",
    "    count_df = pd.DataFrame(list_of_counts)\n",
    "    count_df.fillna(0, inplace = True)\n",
    "\n",
    "    # make all values ints, when applicable\n",
    "    for i in count_df.columns:\n",
    "        try: count_df[[i]] = count_df[[i]].astype(int)\n",
    "        except: pass\n",
    "\n",
    "    return(count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_count_df(train_img_dicts)\n",
    "val_df = get_count_df(val_img_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person                      1953\n",
       "male                        1633\n",
       "adult                       1503\n",
       "uniformed                    440\n",
       "hostage                       37\n",
       "child                        207\n",
       "blooded_area                  28\n",
       "casualty                      23\n",
       "female                       274\n",
       "elderly                       66\n",
       "religious_garment_female     158\n",
       "firearm                      185\n",
       "youth                        151\n",
       "flag_iraqi                    56\n",
       "infant                        10\n",
       "flag_us                       42\n",
       "military_vehicle              62\n",
       "prayer_salah                   6\n",
       "prayer_informal                2\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N objects\n",
    "train_df.sum(axis=0)[2:] # ignore image_id and file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person                      739\n",
       "male                        669\n",
       "adult                       656\n",
       "uniformed                   234\n",
       "hostage                      21\n",
       "child                       121\n",
       "blooded_area                 19\n",
       "casualty                     22\n",
       "female                      169\n",
       "elderly                      42\n",
       "religious_garment_female    104\n",
       "firearm                     107\n",
       "youth                        87\n",
       "flag_iraqi                   39\n",
       "infant                       10\n",
       "flag_us                      37\n",
       "military_vehicle             54\n",
       "prayer_salah                  2\n",
       "prayer_informal               2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N images containing objects\n",
    "train_df.astype(bool).astype(int).sum(axis=0)[2:] # ignore image_id and file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person                      468\n",
       "child                        44\n",
       "female                       81\n",
       "casualty                      6\n",
       "adult                       369\n",
       "male                        375\n",
       "elderly                      19\n",
       "flag_iraqi                   20\n",
       "uniformed                   114\n",
       "firearm                      50\n",
       "youth                        33\n",
       "military_vehicle             30\n",
       "religious_garment_female     39\n",
       "flag_us                      11\n",
       "blooded_area                  4\n",
       "infant                        1\n",
       "hostage                       2\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N objects\n",
    "val_df.sum(axis=0)[2:] # ignore image_id and file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "person                      190\n",
       "child                        24\n",
       "female                       54\n",
       "casualty                      6\n",
       "adult                       176\n",
       "male                        161\n",
       "elderly                      13\n",
       "flag_iraqi                   14\n",
       "uniformed                    63\n",
       "firearm                      25\n",
       "youth                        18\n",
       "military_vehicle             20\n",
       "religious_garment_female     30\n",
       "flag_us                      10\n",
       "blooded_area                  4\n",
       "infant                        1\n",
       "hostage                       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N images containing objects\n",
    "val_df.astype(bool).astype(int).sum(axis=0)[2:] # ignore image_id and file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist of train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist of objects in full set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist of objects in train, val and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hist of total objects"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7d17cb9a9d1570995e0d0bdc2d11126f4f4f2acc1c9f41118ccdb5e1f5d3dec"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('pytorch_env_new': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
